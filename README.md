# Multi-dimensional Texture Tactile Reproduction Method based on Generative Replay Incremental Learning Model

<h3 align="center">ABSTRACT</h3>

With the continuous expansion of texture sample data, texture tactile models need to maintain stable rendering of learned textures while continuously acquiring new texture features. However, existing models often fail to effectively absorb new knowledge in continuous learning scenarios, or cannot simultaneously balance the learning efficiency of new textures with the maintenance of existing reproduction capabilities. To address this issue, we propose a Texture Tactile Continuous Learning Model (TTCLM) based on generative replay mechanism. This model has added a continuous learning phase on the basis of our initial training phase work. During the continuous learning phase, buffers and adapters are introduced for replay learning, and the backbone network parameters are frozen to avoid new tasks damaging existing knowledge. Through the joint loss optimization of the decoder and discriminator, TTCLM has improved its fitting ability for new tasks. We trained the model on the SENS3 tactile dataset and conducted three systematic validation experiments. The results showed that TTCLM exhibited excellent performance in continuous learning and resistance to catastrophic forgetting (BWT value of -0.101), and demonstrated good robustness under changes in the order of new tasks. Finally, we conducted three user experiments, and the results showed that the proposed tactile continuous learning method can effectively absorb new sample information, enabling users to achieve higher accuracy (88.33%) in virtual and real texture matching tasks. Compared with existing baseline methods, this method achieved the highest perceived average similarity score (7.60), verifying its continuous learning ability and user experience improvement effect in dynamic texture scenes.

<h3 align="center">Texture Tactile Continuous Learning Model</h3>

In the preliminary work [Chen D, Ding Y, Gao P, et al. Multi dimensional Texture Haptic Cross modal Generation and Display Method based on Bi Mamba Network [J]. IEEE Transactions on Instrumentation and Measurement, 2025], we constructed a multidimensional texture tactile rendering model based on multimodal data (texture images and user interaction information) to generate real-time acceleration and friction signals for user interaction with virtual textures. On this basis, this study made structural improvements and introduced a continuous learning strategy based on replay mechanism. The overall architecture is shown in the following figure.

<p align="center">
  <img src="fig1.jpg" alt="fig1" width="400"/>
</p>
